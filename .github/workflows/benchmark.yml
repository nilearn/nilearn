---
# TL;DR: this workflow runs the benchmarks on the latest commit on the main
# and then deploys the updated benchmarks to
# `nilearn.github.io/benchmarks <https://nilearn.github.io/benchmarks/>`_.
# You can trigger it manually by adding ``[bm]`` to the commit message on the
# main. It will also run on a schedule at regular interval.
#
# Step-wise details:
#
# - The benchmarks are set to run at regular intervals via cron.
# - It can also be triggered manually by adding ``[bm]`` to the commit message.
# - First it installs `asv <https://asv.readthedocs.io/en/latest/index.html>`_.
# - Then it sets up the SSH key to access the
#   `benchmarks <https://github.com/nilearn/benchmarks>`_ repo,
#   pulls it, and copies the results dir into the current dir. We will append
#   the new results to these results.
# - ``asv machine --yes`` then fetches the machine info like CPU, RAM, OS,
#   etc. and saves it in ``~/.asv-machine.json``, but it gives a unique name
#   to the machine on every run even if the specs are the same. So we will set
#   a fixed name for the machine so the legend isn't too overcrowded.
# - To do this, we edit the ``~/.asv-machine.json`` file to change the machine
#   name to a fixed name ``fv-az1113-357``. The name is arbitrary and has been
#   chosen to match the first run. The script
#   ``build_tools/github/set_machine_name.py`` does all this.
# - Then we run the benchmarks (``asv run``) such that the results are
#   appended to the old results (via ``--append-samples`` parameter).
#   ``-ev`` makes sure any errors are printed in detail and all the output can
#   be seen in the logs.
# - Then we create the HTML with all the results via ``asv publish``.
# - We upload the results as artifacts so that we can download them later.
# - Then we push the new results back to the
#   `benchmarks <https://github.com/nilearn/benchmarks>`_  repo. This will
#   automatically deploy the new results to
#   `nilearn.github.io/benchmarks <https://nilearn.github.io/benchmarks/>`_.
###
name: Run and deploy Nilearn benchmarks

on:
    push:
        branches:
        -   main
    schedule:
    # everday at 10pm UTC
    -   cron: 0 22 * * 0

jobs:
    benchmark:
        if: >-
            (contains(github.event.head_commit.message, '[bm]') ||
            github.event_name == 'schedule') &&
            github.repository == 'nilearn/nilearn'
        # TODO bump to 24.04 when bumping minimum python to 3.10
        runs-on: ubuntu-22.04
        steps:
        -   uses: actions/checkout@v3
        -   uses: actions/setup-python@v3
        -   name: Install asv
            run: |
                pip install --upgrade pip
                pip install asv
        -   name: Add SSH key for benchmarks repo
            env:
                SSH_AUTH_SOCK: /tmp/ssh_agent_benchmarks.sock
            run: |
                mkdir -p ~/.ssh
                ssh-keyscan github.com >> ~/.ssh/known_hosts
                echo "${{ secrets.UPLOAD_BENCHMARK_RESULTS }}" > ~/.ssh/github_actions
                chmod 600 ~/.ssh/github_actions
                ssh-agent -a $SSH_AUTH_SOCK > /dev/null
                ssh-add ~/.ssh/github_actions
        -   name: Pull previous results from benchmarks repo
            env:
                SSH_AUTH_SOCK: /tmp/ssh_agent_benchmarks.sock
            run: |
                git clone git@github.com:nilearn/benchmarks.git benchmarks_repo
                cp -r benchmarks_repo/results .
        -   name: Get all the machine info
            run: |
                asv machine --yes
        -   name: Edit asv-machine.json to a custom machine name
            run: python ./build_tools/github/set_machine_name.py fv-az1113-357
        -   name: Run all benchmarks on the latest commit
            run: |
                asv run -ev --append-samples --machine fv-az1113-357
        -   name: Create html with all results
            run: |
                asv publish
        -   uses: actions/upload-artifact@v4
            with:
                name: Upload asv benchmark results as artifacts
                path: |
                    ./env
                    ./html
                    ./results
                compression-level: 9
        -   name: Fetch log for this CI run
            run: |
                # Download the current run logs using GitHub API
                curl -L \
                  -H "Accept: application/vnd.github+json" \
                  -H "Authorization: Bearer ${{ secrets.UPLOAD_BENCHMARK_RESULTS }}" \
                  -H "X-GitHub-Api-Version: 2022-11-28" \
                  "https://api.github.com/repos/$GITHUB_REPOSITORY/actions/runs/$GITHUB_RUN_ID/logs" \
                  --output workflow_logs.zip

                # Extract the logs
                mkdir -p workflow_logs
                unzip -o workflow_logs.zip -d workflow_logs/

                # Create a combined log file
                cat workflow_logs/* > logs/log_$GITHUB_RUN_ID || echo "Could not concatenate logs"

                # Clean up
                rm -rf workflow_logs workflow_logs.zip

                echo "Workflow logs downloaded and saved to results directory"
        -   name: Push new results and logs back to benchmarks repo
            env:
                SSH_AUTH_SOCK: /tmp/ssh_agent.sock
            run: |
                cd benchmarks_repo
                cp -r ../results .
                cp -r ../logs/log_$GITHUB_RUN_ID ./logs/log_$GITHUB_RUN_ID
                git config --global user.name "GitHub Actions"
                git config --global user.email "actions@github.com"
                git add .
                git commit -m "Update benchmark results and logs ($GITHUB_RUN_ID)"
                git push origin main
                cd ..
        -   name: Push html to gh-pages branch
            env:
                SSH_AUTH_SOCK: /tmp/ssh_agent.sock
            run: |
                cd benchmarks_repo
                git checkout gh-pages
                cp -r ../html/* .
                git add .
                git commit -m "Update benchmark HTML ($GITHUB_RUN_ID)"
                git push origin gh-pages
