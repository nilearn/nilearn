# Workflow to build the documentation
name: Documentation builder

on:
  push:
    branches:
      - "main"
  pull_request:
    branches:
      - "*"
  schedule:
    - cron: "0 6 * * *"

jobs:
  # Steps to download data and build docs.
  # First checks and restores data from cache if it exists.
  build_docs:
    # This prevents this workflow from running on a fork.
    # To test this workflow on a fork, uncomment the following line.
    if: github.repository == 'nilearn/nilearn'
    runs-on: ubuntu-latest
    env:
      BROWSER: "/usr/bin/firefox"
      DISPLAY: ':99.0'
      NILEARN_DATA: "/home/runner/work/nilearn/nilearn/nilearn_data"
    defaults:
      run:
        shell: bash -el {0}
    steps:
      - name: Source caching
        uses: actions/cache@v3
        with:
          path: .git
          key: source-cache-${{ runner.os }}-${{ github.run_id }}
          restore-keys: |
            source-cache-${{ runner.os }}
      - name: Checkout nilearn
        uses: actions/checkout@v3
        with:
          ref: ${{ github.event.pull_request.head.sha }}
          fetch-depth: 0
      - name: Complete checkout
        run: |
          if ! git remote -v | grep upstream; then
            git remote add upstream https://github.com/nilearn/nilearn.git
          fi
          git fetch upstream
      - name: Determine force download
        run: |
          commit_msg=$(git log -2 --format=oneline);
          echo $commit_msg;
          if [[ $commit_msg == *"[force download]"* ]]; then
            echo "All datasets will be downloaded as requested.";
            touch restore.txt;
          else
            echo "Data cache will be used if available.";
            echo "true" | tee restore.txt;
          fi
      - name: Get cache key
        run: |
          if [[ $(cat restore.txt) == "true" ]]; then
              date +%U > week_num;
          else
              echo "missing" > week_num;
          fi

      # Set up environment
      - name: Merge with upstream
        run: |
          echo $(git log -1 --pretty=%B) | tee gitlog.txt
          echo "gitlog.txt = $(cat gitlog.txt)"
          echo ${GITHUB_REF//*pull\//} | tee merge.txt
          if [[ $(cat merge.txt) != "" ]]; then
              echo "Merging $(cat merge.txt)";
              git pull --ff-only upstream "refs/pull/$(cat merge.txt)";
          fi
      - name: Install apt packages
        run: |
          ./build_tools/github/build_docs_apt_dependencies.sh
      - name: Setup conda
        uses: conda-incubator/setup-miniconda@v2
        with:
          auto-activate-base: true
          activate-environment: ''
          miniconda-version: 'latest'
          channels: conda-forge
      - name: Install packages in conda env
        run: |
          ./build_tools/github/build_docs_dependencies.sh

      - name: Data caching
        uses: actions/cache@v3
        with:
          path: |
           $NILEARN_DATA/basc_multiscale_2015  # <  1 MB
           $NILEARN_DATA/destrieux_surface     # <  1 MB
           $NILEARN_DATA/difumo_atlases        # ~  4 MB
           $NILEARN_DATA/fsaverage             # ~ 28 MB
           $NILEARN_DATA/Megatrawls            # ~  9 MB
           $NILEARN_DATA/msdl_atlas            # ~ 21 MB
           $NILEARN_DATA/neurovault            # ~ 13 MB
           $NILEARN_DATA/pauli_2017            # ~  1 MB
           $NILEARN_DATA/yeo_2011              # ~  3 MB
          key: v1-small-cache-${{ hashFiles('**/week_num**') }}
      - name: Data caching
        uses: actions/cache@v3
        with:
          path: |
           $NILEARN_DATA/adhd                 # ~ 45 MB
          key: v1-adhd-${{ hashFiles('**/week_num**') }}
      - name: Data caching
        uses: actions/cache@v3
        with:
          path: |
           $NILEARN_DATA/allen_rsn_2011        # ~ 32 MB
          key: v1-allen_rsn_2011-${{ hashFiles('**/week_num**') }}
      - name: Data caching
        uses: actions/cache@v3
        with:
          path: |
           $NILEARN_DATA/brainomics_localizer  # ~ 66 MB
          key: v1-brainomics_localizer-${{ hashFiles('**/week_num**') }}
      - name: Data caching
        uses: actions/cache@v3
        with:
          path: |
           $NILEARN_DATA/development_fmri      # ~373 MB
          key: v1-development_fmri-${{ hashFiles('**/week_num**') }}
      - name: Data caching
        uses: actions/cache@v3
        with:
          path: |
           $NILEARN_DATA/ds000030              # ~832 MB
          key: v1-ds000030-${{ hashFiles('**/week_num**') }}
      - name: Data caching
        uses: actions/cache@v3
        with:
          path: |
           $NILEARN_DATA/fiac_nilearn.glm      # ~ 82 MB
          key: v1-fiac_nilearn.glm-${{ hashFiles('**/week_num**') }}
      - name: Data caching
        uses: actions/cache@v3
        with:
          path: |
           $NILEARN_DATA/fMRI-language-localizer-demo-dataset # ~ 750 MB
          key: v1-fMRI-language-localizer-demo-dataset-${{ hashFiles('**/week_num**') }}
      - name: Data caching
        uses: actions/cache@v3
        with:
          path: |
           $NILEARN_DATA/fsl                   # ~ 30 MB
          key: v1-fsl-${{ hashFiles('**/week_num**') }}
      - name: Data caching
        uses: actions/cache@v3
        with:
          path: |
           $NILEARN_DATA/haxby2001             # ~302 MB
          key: v1-haxby2001-${{ hashFiles('**/week_num**') }}
      - name: Data caching
        uses: actions/cache@v3
        with:
          path: |
           $NILEARN_DATA/icbm152_2009          # ~ 63 MB
          key: v1-icbm152_2009-${{ hashFiles('**/week_num**') }}
      - name: Data caching
        uses: actions/cache@v3
        with:
          path: |
           $NILEARN_DATA/jimura_poldrack_2012_zmaps # ~ 105 MB
          key: v1-jimura_poldrack_2012_zmaps-${{ hashFiles('**/week_num**') }}
      - name: Data caching
        uses: actions/cache@v3
        with:
          path: |
           $NILEARN_DATA/localizer_first_level # ~ 35 MB
          key: v1-localizer_first_level-${{ hashFiles('**/week_num**') }}
      - name: Data caching
        uses: actions/cache@v3
        with:
          path: |
           $NILEARN_DATA/miyawaki2008          # ~181 MB
          key: v1-miyawaki2008-${{ hashFiles('**/week_num**') }}
      - name: Data caching
        uses: actions/cache@v3
        with:
          path: |
           $NILEARN_DATA/nki_enhanced_surface  # ~ 85 MB
          key: v1-nki_enhanced_surface-${{ hashFiles('**/week_num**') }}
      - name: Data caching
        uses: actions/cache@v3
        with:
          path: |
           $NILEARN_DATA/oasis1                # ~913 MB
          key: v1-oasis1-${{ hashFiles('**/week_num**') }}
      - name: Data caching
        uses: actions/cache@v3
        with:
          path: |
           $NILEARN_DATA/smith_2009            # ~171 MB
          key: v1-smith_2009-${{ hashFiles('**/week_num**') }}
      - name: Data caching
        uses: actions/cache@v3
        with:
          path: |
           $NILEARN_DATA/spm_auditory          # ~230 MB
          key: v1-spm_auditory-${{ hashFiles('**/week_num**') }}
      - name: Data caching
        uses: actions/cache@v3
        with:
          path: |
           $NILEARN_DATA/spm_multimodal_fmri   # ~227 MB
          key: v1-spm_multimodal_fmri-${{ hashFiles('**/week_num**') }}

      # Download data and cache if it is not restored in previous steps, then
      # run doc build
      - name: Fetch and download nilearn data
        id: download_data
        run: |
          source activate testenv
          echo "Conda active env = $CONDA_DEFAULT_ENV";
          ./build_tools/github/download_data.sh
#      - name: Setup tmate session
#        uses: mxschmitt/action-tmate@v3
#        with:
#          limit-access-to-actor: true
      - name: Find build type
        run: |
          ./build_tools/github/build_type.sh
      - name: Verify build type
        run: |
          echo "PATTERN = $(cat pattern.txt)"
          echo "BUILD = $(cat build.txt)"
      - name: Set up display server for virtual browser
        run: |
          Xvfb -ac :99 -screen 0 1280x1024x16 > /dev/null 2>&1 &
      - name: Build docs
        run: |
          source activate testenv
          echo "Conda active env = $CONDA_DEFAULT_ENV";
          cd doc;
          set -o pipefail;
          PATTERN=$(cat ../pattern.txt) make $(cat ../build.txt) 2>&1 | tee log.txt;
      - name: Upload documentation
        uses: actions/upload-artifact@v3
        with:
          name: doc
          path: doc/_build/html
