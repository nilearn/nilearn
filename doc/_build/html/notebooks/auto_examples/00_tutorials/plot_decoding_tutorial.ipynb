{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# A introduction tutorial to fMRI decoding\n\nHere is a simple tutorial on decoding with nilearn. It reproduces the\nHaxby 2001 study on a face vs cat discrimination task in a mask of the\nventral stream.\n\n    * J.V. Haxby et al. \"Distributed and Overlapping Representations of Faces\n      and Objects in Ventral Temporal Cortex\", Science vol 293 (2001), p\n      2425.-2430.\n\nThis tutorial is meant as an introduction to the various steps of a decoding\nanalysis using Nilearn meta-estimator: :class:`nilearn.decoding.Decoder`\n\nIt is not a minimalistic example, as it strives to be didactic. It is not\nmeant to be copied to analyze new data: many of the steps are unnecessary.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Retrieve and load the fMRI data from the Haxby study\n\n### First download the data\n\nThe :func:`nilearn.datasets.fetch_haxby` function will download the\nHaxby dataset if not present on the disk, in the nilearn data directory.\nIt can take a while to download about 310 Mo of data from the Internet.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn import datasets\n# By default 2nd subject will be fetched\nhaxby_dataset = datasets.fetch_haxby()\n# 'func' is a list of filenames: one for each subject\nfmri_filename = haxby_dataset.func[0]\n\n# print basic information on the dataset\nprint('First subject functional nifti images (4D) are at: %s' %\n      fmri_filename)  # 4D data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualizing the fmri volume\n\nOne way to visualize a :term:`fmri<fMRI>` volume is\nusing :func:`nilearn.plotting.plot_epi`.\nWe will visualize the previously fetched :term:`fmri<fMRI>` data from Haxby dataset.\n\nBecause :term:`fmri<fMRI>` data are 4D (they consist of many 3D EPI images), we cannot\nplot them directly using :func:`nilearn.plotting.plot_epi` (which accepts\njust 3D input). Here we are using :func:`nilearn.image.mean_img` to\nextract a single 3D EPI image from the :term:`fmri<fMRI>` data.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn import plotting\nfrom nilearn.image import mean_img\nplotting.view_img(mean_img(fmri_filename), threshold=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feature extraction: from fMRI volumes to a data matrix\n\nThese are some really lovely images, but for machine learning\nwe need matrices to work with the actual data. Fortunately, the\n:class:`nilearn.decoding.Decoder` object we will use later on can\nautomatically transform Nifti images into matrices.\nAll we have to do for now is define a mask filename.\n\nA mask of the Ventral Temporal (VT) cortex coming from the\nHaxby study is available:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mask_filename = haxby_dataset.mask_vt[0]\n\n# Let's visualize it, using the subject's anatomical image as a\n# background\nplotting.plot_roi(mask_filename, bg_img=haxby_dataset.anat[0],\n                  cmap='Paired')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load the behavioral labels\n\nNow that the brain images are converted to a data matrix, we can apply\nmachine-learning to them, for instance to predict the task that the subject\nwas doing. The behavioral labels are stored in a CSV file, separated by\nspaces.\n\nWe use pandas to load them in an array.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n# Load behavioral information\nbehavioral = pd.read_csv(haxby_dataset.session_target[0], delimiter=' ')\nprint(behavioral)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The task was a visual-recognition task, and the labels denote the\nexperimental condition: the type of object that was presented to the\nsubject. This is what we are going to try to predict.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "conditions = behavioral['labels']\nprint(conditions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Restrict the analysis to cats and faces\n\nAs we can see from the targets above, the experiment contains many\nconditions. As a consequence, the data is quite big. Not all of this data\nhas an interest to us for decoding, so we will keep only :term:`fmri<fMRI>` signals\ncorresponding to faces or cats. We create a mask of the samples belonging to\nthe condition; this mask is then applied to the :term:`fmri<fMRI>` data to restrict the\nclassification to the face vs cat discrimination.\n\nThe input data will become much smaller (i.e. :term:`fmri<fMRI>` signal is shorter):\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "condition_mask = conditions.isin(['face', 'cat'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Because the data is in one single large 4D image, we need to use\nindex_img to do the split easily.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn.image import index_img\nfmri_niimgs = index_img(fmri_filename, condition_mask)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We apply the same mask to the targets\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "conditions = conditions[condition_mask]\n# Convert to numpy array\nconditions = conditions.values\nprint(conditions.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Decoding with Support Vector Machine\n\nAs a decoder, we use a Support Vector Classifier with a linear kernel. We\nfirst create it using by using :class:`nilearn.decoding.Decoder`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn.decoding import Decoder\ndecoder = Decoder(estimator='svc', mask=mask_filename, standardize=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The decoder object is an object that can be fit (or trained) on data with\nlabels, and then predict labels on data without.\n\nWe first fit it on the data\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "decoder.fit(fmri_niimgs, conditions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can then predict the labels from the data\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "prediction = decoder.predict(fmri_niimgs)\nprint(prediction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that for this classification task both classes contain the same number\nof samples (the problem is balanced). Then, we can use accuracy to measure\nthe performance of the decoder. This is done by defining accuracy as the\n`scoring`.\nLet's measure the prediction accuracy:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print((prediction == conditions).sum() / float(len(conditions)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This prediction accuracy score is meaningless. Why?\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Measuring prediction scores using cross-validation\n\nThe proper way to measure error rates or prediction accuracy is via\ncross-validation: leaving out some data and testing on it.\n\n### Manually leaving out data\n\nLet's leave out the 30 last data points during training, and test the\nprediction on these 30 last points:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fmri_niimgs_train = index_img(fmri_niimgs, slice(0, -30))\nfmri_niimgs_test = index_img(fmri_niimgs, slice(-30, None))\nconditions_train = conditions[:-30]\nconditions_test = conditions[-30:]\n\ndecoder = Decoder(estimator='svc', mask=mask_filename, standardize=True)\ndecoder.fit(fmri_niimgs_train, conditions_train)\n\nprediction = decoder.predict(fmri_niimgs_test)\n\n# The prediction accuracy is calculated on the test data: this is the accuracy\n# of our model on examples it hasn't seen to examine how well the model perform\n# in general.\n\nprint(\"Prediction Accuracy: {:.3f}\".format(\n    (prediction == conditions_test).sum() / float(len(conditions_test))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Implementing a KFold loop\n\nWe can manually split the data in train and test set repetitively in a\n`KFold` strategy by importing scikit-learn's object:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold\ncv = KFold(n_splits=5)\n\n# The \"cv\" object's split method can now accept data and create a\n# generator which can yield the splits.\nfold = 0\nfor train, test in cv.split(conditions):\n    fold += 1\n    decoder = Decoder(estimator='svc', mask=mask_filename, standardize=True)\n    decoder.fit(index_img(fmri_niimgs, train), conditions[train])\n    prediction = decoder.predict(index_img(fmri_niimgs, test))\n    print(\n        \"CV Fold {:01d} | Prediction Accuracy: {:.3f}\".format(\n            fold,\n            (prediction == conditions[test]).sum() / float(len(\n                conditions[test]))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cross-validation with the decoder\n\nThe decoder also implements a cross-validation loop by default and returns\nan array of shape (cross-validation parameters, `n_folds`). We can use\naccuracy score to measure its performance by defining `accuracy` as the\n`scoring` parameter.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_folds = 5\ndecoder = Decoder(\n    estimator='svc', mask=mask_filename,\n    standardize=True, cv=n_folds,\n    scoring='accuracy'\n)\ndecoder.fit(fmri_niimgs, conditions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cross-validation pipeline can also be implemented manually. More details can\nbe found on [scikit-learn website](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html).\n\nThen we can check the best performing parameters per fold.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(decoder.cv_params_['face'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p></p></div>\n\tWe can speed things up to use all the CPUs of our computer with the\n\tn_jobs parameter.\n\nThe best way to do cross-validation is to respect the structure of\nthe experiment, for instance by leaving out full sessions of\nacquisition.\n\nThe number of the session is stored in the CSV file giving the\nbehavioral data. We have to apply our session mask, to select only cats\nand faces.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "session_label = behavioral['chunks'][condition_mask]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The :term:`fMRI` data is acquired by sessions, and the noise is autocorrelated in a\ngiven session. Hence, it is better to predict across sessions when doing\ncross-validation. To leave a session out, pass the cross-validator object\nto the cv parameter of decoder.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import LeaveOneGroupOut\ncv = LeaveOneGroupOut()\n\ndecoder = Decoder(estimator='svc', mask=mask_filename, standardize=True,\n                  cv=cv)\ndecoder.fit(fmri_niimgs, conditions, groups=session_label)\n\nprint(decoder.cv_scores_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inspecting the model weights\n\nFinally, it may be useful to inspect and display the model weights.\n\n### Turning the weights into a nifti image\n\nWe retrieve the SVC discriminating weights\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "coef_ = decoder.coef_\nprint(coef_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "It's a numpy array with only one coefficient per voxel:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(coef_.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To get the Nifti image of these coefficients, we only need retrieve the\n`coef_img_` in the decoder and select the class\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "coef_img = decoder.coef_img_['face']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "coef_img is now a NiftiImage.  We can save the coefficients as a nii.gz file:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "decoder.coef_img_['face'].to_filename('haxby_svc_weights.nii.gz')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plotting the SVM weights\n\nWe can plot the weights, using the subject's anatomical as a background\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plotting.view_img(\n    decoder.coef_img_['face'], bg_img=haxby_dataset.anat[0],\n    title=\"SVM weights\", dim=-1\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What is the chance level accuracy?\n\nDoes the model above perform better than chance?\nTo answer this question, we measure a score at random using simple strategies\nthat are implemented in the :class:`nilearn.decoding.Decoder` object. This is\nuseful to inspect the decoding performance by comparing to a score at chance.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's define a object with Dummy estimator replacing 'svc' for classification\nsetting. This object initializes estimator with default dummy strategy.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dummy_decoder = Decoder(estimator='dummy_classifier', mask=mask_filename,\n                        cv=cv)\ndummy_decoder.fit(fmri_niimgs, conditions, groups=session_label)\n\n# Now, we can compare these scores by simply taking a mean over folds\nprint(dummy_decoder.cv_scores_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Further reading\n\n* The `section of the documentation on decoding <decoding>`\n\n* `sphx_glr_auto_examples_02_decoding_plot_haxby_anova_svm.py`\n  For decoding without a precomputed mask\n\n* `frem`\n\n* `space_net`\n\n______________\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}