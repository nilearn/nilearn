{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Regions extraction using dictionary learning and functional connectomes\n\nThis example shows how to use :class:`nilearn.regions.RegionExtractor`\nto extract spatially constrained brain regions from whole brain maps decomposed\nusing :term:`Dictionary learning` and use them to build\na :term:`functional connectome`.\n\nWe used 20 movie-watching functional datasets from\n:func:`nilearn.datasets.fetch_development_fmri` and\n:class:`nilearn.decomposition.DictLearning` for set of brain atlas maps.\n\nThis example can also be inspired to apply the same steps to even regions extraction\nusing :term:`ICA` maps. In that case, idea would be to replace\n:term:`Dictionary learning` to canonical :term:`ICA` decomposition\nusing :class:`nilearn.decomposition.CanICA`\n\nPlease see the related documentation of :class:`nilearn.regions.RegionExtractor`\nfor more details.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fetch brain development functional datasets\n\nWe use nilearn's datasets downloading utilities\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn import datasets\n\nrest_dataset = datasets.fetch_development_fmri(n_subjects=20)\nfunc_filenames = rest_dataset.func\nconfounds = rest_dataset.confounds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extract functional networks with :term:`Dictionary learning`\n\nImport :class:`~nilearn.decomposition.DictLearning` from the\n:mod:`~nilearn.decomposition` module, instantiate the object, and\n:meth:`~nilearn.decomposition.DictLearning.fit` the model to the\nfunctional datasets\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn.decomposition import DictLearning\n\n# Initialize DictLearning object\ndict_learn = DictLearning(n_components=8, smoothing_fwhm=6.,\n                          memory=\"nilearn_cache\", memory_level=2,\n                          random_state=0)\n# Fit to the data\ndict_learn.fit(func_filenames)\n# Resting state networks/maps in attribute `components_img_`\ncomponents_img = dict_learn.components_img_\n\n# Visualization of functional networks\n# Show networks using plotting utilities\nfrom nilearn import plotting\n\nplotting.plot_prob_atlas(components_img, view_type='filled_contours',\n                         title='Dictionary Learning maps')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extract regions from networks\n\nImport :class:`~nilearn.regions.RegionExtractor` from the\n:mod:`~nilearn.regions` module.\n``threshold=0.5`` indicates that we keep nominal of amount nonzero\n:term:`voxels<voxel>` across all maps, less the threshold means that\nmore intense non-voxels will be survived.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn.regions import RegionExtractor\n\nextractor = RegionExtractor(components_img, threshold=0.5,\n                            thresholding_strategy='ratio_n_voxels',\n                            extractor='local_regions',\n                            standardize=True, min_region_size=1350)\n# Just call fit() to process for regions extraction\nextractor.fit()\n# Extracted regions are stored in regions_img_\nregions_extracted_img = extractor.regions_img_\n# Each region index is stored in index_\nregions_index = extractor.index_\n# Total number of regions extracted\nn_regions_extracted = regions_extracted_img.shape[-1]\n\n# Visualization of region extraction results\ntitle = ('%d regions are extracted from %d components.'\n         '\\nEach separate color of region indicates extracted region'\n         % (n_regions_extracted, 8))\nplotting.plot_prob_atlas(regions_extracted_img, view_type='filled_contours',\n                         title=title)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compute correlation coefficients\n\nFirst we need to do subjects timeseries signals extraction and then estimating\ncorrelation matrices on those signals.\nTo extract timeseries signals, we call\n:meth:`~nilearn.regions.RegionExtractor.transform` onto each subject\nfunctional data stored in ``func_filenames``.\nTo estimate correlation matrices we import connectome utilities from nilearn.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn.connectome import ConnectivityMeasure\n\ncorrelations = []\n# Initializing ConnectivityMeasure object with kind='correlation'\nconnectome_measure = ConnectivityMeasure(kind='correlation')\nfor filename, confound in zip(func_filenames, confounds):\n    # call transform from RegionExtractor object to extract timeseries signals\n    timeseries_each_subject = extractor.transform(filename, confounds=confound)\n    # call fit_transform from ConnectivityMeasure object\n    correlation = connectome_measure.fit_transform([timeseries_each_subject])\n    # saving each subject correlation to correlations\n    correlations.append(correlation)\n\n# Mean of all correlations\nimport numpy as np\nmean_correlations = np.mean(correlations, axis=0).reshape(n_regions_extracted,\n                                                          n_regions_extracted)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot resulting connectomes\n\nFirst we plot the mean of correlation matrices with\n:func:`~nilearn.plotting.plot_matrix`, and we use\n:func:`~nilearn.plotting.plot_connectome` to plot the\nconnectome relations.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "title = 'Correlation between %d regions' % n_regions_extracted\n\n# First plot the matrix\ndisplay = plotting.plot_matrix(mean_correlations, vmax=1, vmin=-1,\n                               colorbar=True, title=title)\n\n# Then find the center of the regions and plot a connectome\nregions_img = regions_extracted_img\ncoords_connectome = plotting.find_probabilistic_atlas_cut_coords(regions_img)\n\nplotting.plot_connectome(mean_correlations, coords_connectome,\n                         edge_threshold='90%', title=title)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot regions extracted for only one specific network\n\nFirst, we plot a network of ``index=4`` without\nregion extraction (left plot).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn import image\n\nimg = image.index_img(components_img, 4)\ncoords = plotting.find_xyz_cut_coords(img)\ndisplay = plotting.plot_stat_map(img, cut_coords=coords, colorbar=False,\n                                 title='Showing one specific network')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, we plot (right side) same network after region extraction to show that\nconnected regions are nicely separated.\nEach brain extracted region is identified as separate color.\n\nFor this, we take the indices of the all regions extracted related to original\nnetwork given as 4.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "regions_indices_of_map3 = np.where(np.array(regions_index) == 4)\n\ndisplay = plotting.plot_anat(cut_coords=coords,\n                             title='Regions from this network')\n\n# Add as an overlay all the regions of index 4\ncolors = 'rgbcmyk'\nfor each_index_of_map3, color in zip(regions_indices_of_map3[0], colors):\n    display.add_overlay(image.index_img(regions_extracted_img, each_index_of_map3),\n                        cmap=plotting.cm.alpha_cmap(color))\n\nplotting.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}