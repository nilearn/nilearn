{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Deriving spatial maps from group fMRI data using ICA and Dictionary Learning\n\nVarious approaches exist to derive spatial maps or networks from\ngroup fmr data. The methods extract distributed brain regions that\nexhibit similar :term:`BOLD` fluctuations over time. Decomposition\nmethods allow for generation of many independent maps simultaneously\nwithout the need to provide a priori information (e.g. seeds or priors.)\n\nThis example will apply two popular decomposition methods, :term:`ICA` and\n:term:`Dictionary learning`, to :term:`fMRI` data measured while children\nand young adults watch movies. The resulting maps will be visualized using\natlas plotting tools.\n\n:term:`CanICA` is an :term:`ICA` method for group-level analysis of :term:`fMRI` data.\nCompared to other strategies, it brings a well-controlled group model, as well as a\nthresholding algorithm controlling for specificity and sensitivity with\nan explicit model of the signal. The reference paper is:\n\n    * G. Varoquaux et al. \"A group model for stable multi-subject ICA on\n      fMRI datasets\", NeuroImage Vol 51 (2010), p. 288-299\n      [preprint](https://hal.inria.fr/hal-00489507/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load brain development fmri dataset\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn import datasets\n\nrest_dataset = datasets.fetch_development_fmri(n_subjects=30)\nfunc_filenames = rest_dataset.func  # list of 4D nifti files for each subject\n\n# print basic information on the dataset\nprint('First functional nifti image (4D) is at: %s' %\n      rest_dataset.func[0])  # 4D data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Apply CanICA on the data\nWe use \"whole-brain-template\" as a strategy to compute the mask,\nas this leads to slightly faster and more reproducible results.\nHowever, the images need to be in :term:`MNI` template space.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn.decomposition import CanICA\n\ncanica = CanICA(n_components=20,\n                memory=\"nilearn_cache\", memory_level=2,\n                verbose=10,\n                mask_strategy='whole-brain-template',\n                random_state=0)\ncanica.fit(func_filenames)\n\n# Retrieve the independent components in brain space. Directly\n# accessible through attribute `components_img_`.\ncanica_components_img = canica.components_img_\n# components_img is a Nifti Image object, and can be saved to a file with\n# the following line:\ncanica_components_img.to_filename('canica_resting_state.nii.gz')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To visualize we plot the outline of all components on one figure\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn.plotting import plot_prob_atlas\n\n# Plot all ICA components together\nplot_prob_atlas(canica_components_img, title='All ICA components')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we plot the map for each :term:`ICA` component separately\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn.image import iter_img\nfrom nilearn.plotting import plot_stat_map, show\n\nfor i, cur_img in enumerate(iter_img(canica_components_img)):\n    plot_stat_map(cur_img, display_mode=\"z\", title=\"IC %d\" % i,\n                  cut_coords=1, colorbar=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compare CanICA to dictionary learning\n:term:`Dictionary learning` is a sparsity based decomposition method\nfor extracting spatial maps. It extracts maps that are naturally sparse\nand usually cleaner than :term:`ICA`. Here, we will compare networks built\nwith :term:`CanICA` to networks built with :term:`Dictionary learning`.\n\n   * Arthur Mensch et al. [Compressed online dictionary learning for fast resting-state fMRI decomposition](https://hal.archives-ouvertes.fr/hal-01271033/),\n     ISBI 2016, Lecture Notes in Computer Science\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create a dictionary learning estimator\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn.decomposition import DictLearning\n\ndict_learning = DictLearning(n_components=20,\n                             memory=\"nilearn_cache\", memory_level=2,\n                             verbose=1,\n                             random_state=0,\n                             n_epochs=1,\n                             mask_strategy='whole-brain-template')\n\nprint('[Example] Fitting dictionary learning model')\ndict_learning.fit(func_filenames)\nprint('[Example] Saving results')\n# Grab extracted components umasked back to Nifti image.\n# Note: For older versions, less than 0.4.1. components_img_\n# is not implemented. See Note section above for details.\ndictlearning_components_img = dict_learning.components_img_\ndictlearning_components_img.to_filename('dictionary_learning_resting_state.nii.gz')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualize the results\n\nFirst plot all DictLearning components together\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plot_prob_atlas(dictlearning_components_img,\n                title='All DictLearning components')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "One plot of each component\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for i, cur_img in enumerate(iter_img(dictlearning_components_img)):\n    plot_stat_map(cur_img, display_mode=\"z\", title=\"Comp %d\" % i,\n                  cut_coords=1, colorbar=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Estimate explained variance per component and plot using matplotlib\n\nThe fitted object `dict_learning` can be used to calculate the score per component\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "scores = dict_learning.score(func_filenames, per_component=True)\n\n# Plot the scores\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom matplotlib.ticker import FormatStrFormatter\n\nplt.figure(figsize=(4, 4))\npositions = np.arange(len(scores))\nplt.barh(positions, scores)\nplt.ylabel('Component #', size=12)\nplt.xlabel('Explained variance', size=12)\nplt.yticks(np.arange(20))\nplt.gca().xaxis.set_major_formatter(FormatStrFormatter('%.3f'))\nplt.tight_layout()\n\nshow()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>To see how to extract subject-level timeseries' from regions\n    created using :term:`Dictionary learning`, see `example Regions\n    extraction using dictionary learning and functional connectomes\n    <sphx_glr_auto_examples_03_connectivity_plot_extract_regions_dictlearning_maps.py>`.</p></div>\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}