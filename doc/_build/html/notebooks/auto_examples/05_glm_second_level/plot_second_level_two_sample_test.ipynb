{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Second-level fMRI model: two-sample test, unpaired and paired\n\nFull step-by-step example of fitting a GLM to perform a second level analysis\nin experimental data and visualizing the results\n\nMore specifically:\n\n1. A sample of n=16 visual activity fMRIs are downloaded.\n\n2. An unpaired, two-sample t-test is applied to the brain maps in order to\nsee the effect of the contrast difference across subjects.\n\n3. A paired, two-sample t-test is applied to the brain maps in order to see\nthe effect of the contrast difference across subjects, considering subject intercepts\n\nThe contrast is between responses to retinotopically distinct\nvertical versus horizontal checkerboards. At the individual level,\nthese stimuli are sometimes used to map the borders of primary visual areas.\nAt the group level, such a mapping is not possible. Yet, we may\nobserve some significant effects in these areas.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pandas as pd\nfrom nilearn import plotting\nfrom nilearn.datasets import fetch_localizer_contrasts\nimport matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fetch dataset\nWe download a list of left vs right button press contrasts from a\nlocalizer dataset.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_subjects = 16\nsample_vertical = fetch_localizer_contrasts(\n    [\"vertical checkerboard\"], n_subjects,\n    get_tmaps=True, legacy_format=False\n)\nsample_horizontal = fetch_localizer_contrasts(\n    [\"horizontal checkerboard\"], n_subjects,\n    get_tmaps=True, legacy_format=False\n)\n\n# Implicitly, there is a one-to-one correspondence between the two samples:\n# the first image of both samples comes from subject S1, the second from subject S2 etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Estimate second level models\nWe define the input maps and the design matrix for the second level model\nand fit it.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "second_level_input = sample_vertical['cmaps'] + sample_horizontal['cmaps']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we model the effect of conditions (sample 1 vs sample 2).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\ncondition_effect = np.hstack(([1] * n_subjects, [- 1] * n_subjects))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The design matrix for the unpaired test doesn't need any more columns\nFor the paired test, we include an intercept for each subject.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "subject_effect = np.vstack((np.eye(n_subjects), np.eye(n_subjects)))\nsubjects = [f'S{i:02d}' for i in range(1, n_subjects + 1)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then assemble those into design matrices\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "unpaired_design_matrix = pd.DataFrame(\n    condition_effect[:, np.newaxis],\n    columns=['vertical vs horizontal'])\n\npaired_design_matrix = pd.DataFrame(\n    np.hstack((condition_effect[:, np.newaxis], subject_effect)),\n    columns=['vertical vs horizontal'] + subjects)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "and plot the designs.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn.plotting import plot_design_matrix\n_, (ax_unpaired, ax_paired) = plt.subplots(1,2, gridspec_kw={'width_ratios': [1, 17]})\nplot_design_matrix(unpaired_design_matrix, rescale=False, ax=ax_unpaired)\nplot_design_matrix(paired_design_matrix, rescale=False, ax=ax_paired)\nax_unpaired.set_title('unpaired design', fontsize=12)\nax_paired.set_title('paired design', fontsize=12)\nplt.tight_layout()\nplotting.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We specify the analysis models and fit them.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn.glm.second_level import SecondLevelModel\n\nsecond_level_model_unpaired = SecondLevelModel().fit(\n    second_level_input, design_matrix=unpaired_design_matrix)\n\nsecond_level_model_paired = SecondLevelModel().fit(\n    second_level_input, design_matrix=paired_design_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Estimating the contrast is simple. To do so, we provide the column\nname of the design matrix. The argument 'output_type' is set to return all\navailable outputs so that we can compare differences in the effect size,\nvariance, and z-score.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "stat_maps_unpaired = second_level_model_unpaired.compute_contrast(\n                                                    'vertical vs horizontal',\n                                                    output_type='all')\n\nstat_maps_paired = second_level_model_paired.compute_contrast(\n                                                'vertical vs horizontal',\n                                                output_type='all')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot the results\nThe two :term:`'effect_size'<Parameter Estimate>` images are essentially\nidentical.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "(stat_maps_unpaired['effect_size'].get_fdata()\n    - stat_maps_paired['effect_size'].get_fdata()).max()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "But the variance in the unpaired image is larger.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plotting.plot_glass_brain(\n    stat_maps_unpaired['effect_variance'], colorbar=True, vmin=0, vmax=6,\n    title='vertical vs horizontal effect variance, unpaired')\n\nplotting.plot_glass_brain(\n    stat_maps_paired['effect_variance'], colorbar=True, vmin=0, vmax=6,\n    title='vertical vs horizontal effect variance, paired')\n\nplotting.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Together, this makes the z_scores from the paired test larger.\nWe threshold the second level contrast and plot it.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "threshold = 3.1  # corresponds to  p < .001, uncorrected\ndisplay = plotting.plot_glass_brain(\n    stat_maps_unpaired['z_score'], threshold=threshold, colorbar=True, plot_abs=False,\n    title='vertical vs horizontal (unc p<0.001)', vmin=0, vmax=6)\n\ndisplay = plotting.plot_glass_brain(\n    stat_maps_paired['z_score'], threshold=threshold, colorbar=True, plot_abs=False,\n    title='vertical vs horizontal (unc p<0.001)', vmin=0, vmax=6)\n\nplotting.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Unsurprisingly, we see activity in the primary visual cortex, both positive\nand negative.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}