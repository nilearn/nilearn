{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Encoding models for visual stimuli from Miyawaki et al. 2008\n\nThis example partly reproduces the encoding model presented in\n    [Visual image reconstruction from human brain activity\n    using a combination of multiscale local image decoders](http://www.cell.com/neuron/abstract/S0896-6273%2808%2900958-6),\n    Miyawaki, Y., Uchida, H., Yamashita, O., Sato, M. A.,\n    Morito, Y., Tanabe, H. C., ... & Kamitani, Y. (2008).\n    Neuron, 60(5), 915-929.\n\nEncoding models try to predict neuronal activity using information from\npresented stimuli, like an image or sound. Where decoding goes from\nbrain data to real-world stimulus, encoding goes the other direction.\n\nWe demonstrate how to build such an **encoding model** in nilearn, predicting\n**fMRI data** from **visual stimuli**, using the dataset from\n[Miyawaki et al., 2008](http://www.cell.com/neuron/abstract/S0896-6273%2808%2900958-6).\n\nParticipants were shown images, which consisted of random 10x10 binary\n(either black or white) pixels, and the corresponding :term:`fMRI` activity\nwas recorded. We will try to predict the activity in each :term:`voxel`\nfrom the binary pixel-values of the presented images. Then we extract the\nreceptive fields for a set of voxels to see which pixel location a\n:term:`voxel` is most sensitive to.\n\nSee also :doc:`plot_miyawaki_reconstruction` for a decoding\napproach for the same dataset.\n\n.. include:: ../../../examples/masker_note.rst\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading the data\nNow we can load the data set:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn.datasets import fetch_miyawaki2008\n\ndataset = fetch_miyawaki2008()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We only use the training data of this study,\nwhere random binary images were shown.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# training data starts after the first 12 files\nfmri_random_runs_filenames = dataset.func[12:]\nstimuli_random_runs_filenames = dataset.label[12:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can use :func:`nilearn.maskers.MultiNiftiMasker` to load the fMRI\ndata, clean and mask it.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nfrom nilearn.maskers import MultiNiftiMasker\n\nmasker = MultiNiftiMasker(mask_img=dataset.mask, detrend=True,\n                          standardize=True)\nmasker.fit()\nfmri_data = masker.transform(fmri_random_runs_filenames)\n\n# shape of the binary (i.e. black and wihte values) image in pixels\nstimulus_shape = (10, 10)\n\n# We load the visual stimuli from csv files\nstimuli = []\nfor stimulus_run in stimuli_random_runs_filenames:\n    stimuli.append(np.reshape(np.loadtxt(stimulus_run,\n                              dtype=int, delimiter=','),\n                              (-1,) + stimulus_shape, order='F'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's take a look at some of these binary images:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pylab as plt\nplt.figure(figsize=(8, 4))\nplt.subplot(1, 2, 1)\nplt.imshow(stimuli[0][124], interpolation='nearest', cmap='gray')\nplt.axis('off')\nplt.title('Run {}, Stimulus {}'.format(1, 125))\nplt.subplot(1, 2, 2)\nplt.imshow(stimuli[2][101], interpolation='nearest', cmap='gray')\nplt.axis('off')\nplt.title('Run {}, Stimulus {}'.format(3, 102))\nplt.subplots_adjust(wspace=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We now stack the fmri and stimulus data and remove an offset in the\nbeginning/end.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fmri_data = np.vstack([fmri_run[2:] for fmri_run in fmri_data])\nstimuli = np.vstack([stimuli_run[:-2] for stimuli_run in stimuli]).astype(float)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "fmri_data is a matrix of *samples* x *voxels*\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(fmri_data.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We flatten the last two dimensions of stimuli\nso it is a matrix of *samples* x *pixels*.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Flatten the stimuli\nstimuli = np.reshape(stimuli, (-1, stimulus_shape[0] * stimulus_shape[1]))\n\nprint(stimuli.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Building the encoding models\nWe can now proceed to build a simple **voxel-wise encoding model** using\n[Ridge regression](http://en.wikipedia.org/wiki/Tikhonov_regularization).\nFor each voxel we fit an independent regression model,\nusing the pixel-values of the visual stimuli to predict the neuronal\nactivity in this voxel.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Ridge\nfrom sklearn.model_selection import KFold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Using 10-fold cross-validation, we partition the data into 10 'folds'.\nWe hold out each fold of the data for testing, then fit a ridge regression\nto the remaining 9/10 of the data, using stimuli as predictors\nand fmri_data as targets, and create predictions for the held-out 10th.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import r2_score\n\nestimator = Ridge(alpha=100.)\ncv = KFold(n_splits=10)\n\nscores = []\nfor train, test in cv.split(X=stimuli):\n    # we train the Ridge estimator on the training set\n    # and predict the fMRI activity for the test set\n    predictions = Ridge(alpha=100.).fit(\n    stimuli.reshape(-1, 100)[train], fmri_data[train]).predict(\n        stimuli.reshape(-1, 100)[test])\n    # we compute how much variance our encoding model explains in each voxel\n    scores.append(r2_score(fmri_data[test], predictions,\n                           multioutput='raw_values'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Mapping the encoding scores on the brain\nTo plot the scores onto the brain, we create a Nifti1Image containing\nthe scores and then threshold it:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn.image import threshold_img\ncut_score = np.mean(scores, axis=0)\ncut_score[cut_score < 0] = 0\n\n# bring the scores into the shape of the background brain\nscore_map_img = masker.inverse_transform(cut_score)\n\nthresholded_score_map_img = threshold_img(score_map_img, threshold=1e-6, copy=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plotting the statistical map on a background brain, we mark four voxels\nwhich we will inspect more closely later on.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn.plotting import plot_stat_map\nfrom nilearn.image import coord_transform\n\ndef index_to_xy_coord(x, y, z=10):\n    '''Transforms data index to coordinates of the background + offset'''\n    coords = coord_transform(x, y, z,\n                             affine=thresholded_score_map_img.affine)\n    return np.array(coords)[np.newaxis, :] + np.array([0, 1, 0])\n\n\nxy_indices_of_special_voxels = [(30, 10), (32, 10), (31, 9), (31, 10)]\n\ndisplay = plot_stat_map(thresholded_score_map_img, bg_img=dataset.background,\n                        cut_coords=[-8], display_mode='z', aspect=1.25,\n                        title='Explained variance per voxel')\n\n# creating a marker for each voxel and adding it to the statistical map\n\nfor i, (x, y) in enumerate(xy_indices_of_special_voxels):\n    display.add_markers(index_to_xy_coord(x, y), marker_color='none',\n                        edgecolor=['b', 'r', 'magenta', 'g'][i],\n                        marker_size=140, marker='s',\n                        facecolor='none', lw=4.5)\n\n\n# re-set figure size after construction so colorbar gets rescaled too\nfig = plt.gcf()\nfig.set_size_inches(12, 12)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Estimating receptive fields\nNow we take a closer look at the receptive fields of the four marked voxels.\nA voxel's [receptive field](http://en.wikipedia.org/wiki/Receptive_field)\nis the region of a stimulus (like an image) where the presence of an object,\nlike a white instead of a black pixel, results in a change in activity\nin the voxel. In our case the receptive field is just the vector of 100\nregression  coefficients (one for each pixel) reshaped into the 10x10\nform of the original images. Some voxels are receptive to only very few\npixels, so we use [Lasso regression](http://en.wikipedia.org/wiki/Lasso_(statistics)) to estimate a sparse\nset of regression coefficients.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LassoLarsCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\n\n# automatically estimate the sparsity by cross-validation\n\nlasso = make_pipeline(StandardScaler(),\n                      LassoLarsCV(normalize=False, max_iter=10))\n\n# Mark the same pixel in each receptive field\nmarked_pixel = (4, 2)\n\nfrom matplotlib import gridspec\nfrom matplotlib.patches import Rectangle\n\nfig = plt.figure(figsize=(12, 8))\nfig.suptitle('Receptive fields of the marked voxels', fontsize=25)\n\n# GridSpec allows us to do subplots with more control of the spacing\ngs1 = gridspec.GridSpec(2, 3)\n\n# we fit the Lasso for each of the three voxels of the upper row\nfor i, index in enumerate([1780, 1951, 2131]):\n    ax = plt.subplot(gs1[0, i])\n    lasso.fit(stimuli, fmri_data[:, index])\n    # we reshape the coefficients into the form of the original images\n    rf = lasso.named_steps['lassolarscv'].coef_.reshape((10, 10))\n    # add a black background\n    ax.imshow(np.zeros_like(rf), vmin=0., vmax=1., cmap='gray')\n    ax_im = ax.imshow(np.ma.masked_less(rf, 0.1), interpolation=\"nearest\",\n                      cmap=['Blues', 'Greens', 'Reds'][i], vmin=0., vmax=0.75)\n    # add the marked pixel\n    ax.add_patch(Rectangle(\n        (marked_pixel[1] - .5, marked_pixel[0] - .5), 1, 1,\n        facecolor='none', edgecolor='r', lw=4))\n    plt.axis('off')\n    plt.colorbar(ax_im, ax=ax)\n\n# and then for the voxel at the bottom\n\ngs1.update(left=0., right=1., wspace=0.1)\nax = plt.subplot(gs1[1, 1])\nlasso.fit(stimuli, fmri_data[:, 1935])\n# we reshape the coefficients into the form of the original images\nrf = lasso.named_steps['lassolarscv'].coef_.reshape((10, 10))\nax.imshow(np.zeros_like(rf), vmin=0., vmax=1., cmap='gray')\nax_im = ax.imshow(np.ma.masked_less(rf, 0.1), interpolation=\"nearest\",\n                  cmap='RdPu', vmin=0., vmax=0.75)\n\n# add the marked pixel\nax.add_patch(Rectangle(\n    (marked_pixel[1] - .5, marked_pixel[0] - .5), 1, 1,\n    facecolor='none', edgecolor='r', lw=4))\nplt.axis('off')\nplt.colorbar(ax_im, ax=ax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The receptive fields of the four voxels are not only close to each other,\nthe relative location of the pixel each voxel is most sensitive to\nroughly maps to the relative location of the voxels to each other.\nWe can see a relationship between some voxel's receptive field and\nits location in the brain.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}