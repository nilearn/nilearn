{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# BIDS dataset first and second level analysis\n\n\nFull step-by-step example of fitting a :term:`GLM` to perform a first and second level\nanalysis in a :term:`BIDS` dataset and visualizing the results.\nDetails about the :term:`BIDS` standard can be consulted at\n[http://bids.neuroimaging.io/](http://bids.neuroimaging.io/).\n\nMore specifically:\n\n1. Download an :term:`fMRI` :term:`BIDS` dataset with two language conditions to contrast.\n2. Extract first level model objects automatically from the :term:`BIDS` dataset.\n3. Fit a second level model on the fitted first level models. Notice that\n   in this case the preprocessed :term:`bold<BOLD>` images were already normalized to the\n   same :term:`MNI` space.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fetch example BIDS dataset\nWe download a simplified :term:`BIDS` dataset made available for illustrative\npurposes. It contains only the necessary\ninformation to run a statistical analysis using Nilearn. The raw data\nsubject folders only contain bold.json and events.tsv files, while the\nderivatives folder includes the preprocessed files preproc.nii and the\nconfounds.tsv files.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn.datasets import fetch_language_localizer_demo_dataset\ndata_dir, _ = fetch_language_localizer_demo_dataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here is the location of the dataset on disk.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(data_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Obtain automatically FirstLevelModel objects and fit arguments\nFrom the dataset directory we automatically obtain the FirstLevelModel objects\nwith their subject_id filled from the :term:`BIDS` dataset. Moreover, we obtain\nfor each model a dictionary with run_imgs, events and confounder regressors\nsince in this case a confounds.tsv file is available in the :term:`BIDS` dataset.\nTo get the first level models we only have to specify the dataset directory\nand the task_label as specified in the file names.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn.glm.first_level import first_level_from_bids\ntask_label = 'languagelocalizer'\nmodels, models_run_imgs, models_events, models_confounds = \\\n    first_level_from_bids(\n        data_dir, task_label,\n        img_filters=[('desc', 'preproc')])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quick sanity check on fit arguments\nAdditional checks or information extraction from pre-processed data can\nbe made here.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We just expect one run_img per subject.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nprint([os.path.basename(run) for run in models_run_imgs[0]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The only confounds stored are regressors obtained from motion correction. As\nwe can verify from the column headers of the confounds table corresponding\nto the only run_img present.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(models_confounds[0][0].columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "During this acquisition the subject read blocks of sentences and\nconsonant strings. So these are our only two conditions in events.\nWe verify there are 12 blocks for each condition.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(models_events[0][0]['trial_type'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## First level model estimation\nNow we simply fit each first level model and plot for each subject the\n:term:`contrast` that reveals the language network (language - string).\nNotice that we can define a contrast using the names of the conditions\nspecified in the events dataframe.\nSum, subtraction and scalar multiplication are allowed.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set the threshold as the z-variate with an uncorrected p-value of 0.001.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from scipy.stats import norm\np001_unc = norm.isf(0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prepare figure for concurrent plot of individual maps.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn import plotting\nimport matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(nrows=2, ncols=5, figsize=(8, 4.5))\nmodel_and_args = zip(models, models_run_imgs, models_events, models_confounds)\nfor midx, (model, imgs, events, confounds) in enumerate(model_and_args):\n    # fit the GLM\n    model.fit(imgs, events, confounds)\n    # compute the contrast of interest\n    zmap = model.compute_contrast('language-string')\n    plotting.plot_glass_brain(zmap, colorbar=False, threshold=p001_unc,\n                              title=('sub-' + model.subject_label),\n                              axes=axes[int(midx / 5), int(midx % 5)],\n                              plot_abs=False, display_mode='x')\nfig.suptitle('subjects z_map language network (unc p<0.001)')\nplotting.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Second level model estimation\nWe just have to provide the list of fitted FirstLevelModel objects\nto the SecondLevelModel object for estimation. We can do this because\nall subjects share a similar design matrix (same variables reflected in\ncolumn names).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn.glm.second_level import SecondLevelModel\nsecond_level_input = models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that we apply a smoothing of 8mm.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "second_level_model = SecondLevelModel(smoothing_fwhm=8.0)\nsecond_level_model = second_level_model.fit(second_level_input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Computing contrasts at the second level is as simple as at the first level.\nSince we are not providing confounders we are performing a one-sample test\nat the second level with the images determined by the specified first level\ncontrast.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "zmap = second_level_model.compute_contrast(\n    first_level_contrast='language-string')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The group level contrast reveals a left lateralized fronto-temporal\nlanguage network.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "plotting.plot_glass_brain(zmap, colorbar=True, threshold=p001_unc,\n                          title='Group language network (unc p<0.001)',\n                          plot_abs=False, display_mode='x')\nplotting.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}