{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Computing a Region of Interest (ROI) mask manually\n\nThis example shows manual steps to create and further modify an ROI spatial\nmask. They represent a means for \"data folding\", i.e., extracting and then\nanalyzing brain data from a subset of voxels rather than whole brain images.\nExample can also help alleviate curse of dimensionality (i.e., statistical\nproblems that arise in the context of high-dimensional input variables).\n\nWe demonstrate how to compute a ROI mask using **T-test** and then how simple\nimage operations can be used before and after computing ROI to improve the\nquality of the computed mask.\n\nThese chains of operations are easy to set up using Nilearn and Scipy Python\nlibraries. Here we give clear guidelines about these steps, starting with\npre-image operations to post-image operations. The main point is that\nvisualization & results checking be possible at each step.\n\nSee also :doc:`plot_extract_rois_smith_atlas` for automatic ROI extraction\nof brain connected networks given in 4D image.\n\n.. include:: ../../../examples/masker_note.rst\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Coordinates of the slice we are interested in each direction. We will be\nusing them for visualization.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# cut in x-direction\nsagittal = -25\n# cut in y-direction\ncoronal = -37\n# cut in z-direction\naxial = -6\n\n# coordinates displaying should be prepared as a list\ncut_coords = [sagittal, coronal, axial]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading the data\nWe rely on the Haxby datasets and its experiments to demonstrate the complete\nlist of operations. Fetching datasets is easy, shipping with Nilearn using a\nfunction named as `fetch_haxby`. The data will then be automatically stored\nin a home directory with \"nilearn_data\" folder in your computer. From which,\nwe process data using paths of the Nifti images.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# We load data from nilearn by import datasets\nfrom nilearn import datasets\n\n# First, we fetch single subject specific data with haxby datasets: to have\n# anatomical image, EPI images and masks images\nhaxby_dataset = datasets.fetch_haxby()\n\n# print basic information on the dataset\nprint('First subject anatomical nifti image (3D) located is at: %s' %\n      haxby_dataset.anat[0])\nprint('First subject functional nifti image (4D) is located at: %s' %\n      haxby_dataset.func[0])\nprint('Labels of haxby dataset (text file) is located at: %s' %\n      haxby_dataset.session_target[0])\n\n# Second, load the labels stored in a text file into array using pandas\nimport pandas as pd\n\nsession_target = pd.read_csv(haxby_dataset.session_target[0], sep=\" \")\n# Now, we have the labels and will be useful while computing student's t-test\nhaxby_labels = session_target['labels']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We have the datasets in hand especially paths to the locations. Now, we do\nsimple pre-processing step called as image smoothing on functional images\nand then build a statistical test on smoothed images.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build a statistical test to find voxels of interest\n**Smoothing**: Functional MRI data have a low signal-to-noise ratio.\nWhen using methods that are not robust to noise, it is useful to apply a\nspatial filtering kernel on the data. Such data smoothing is usually applied\nusing a Gaussian function with 4mm to 12mm\n:term:`full-width at half-maximum<FWHM>` (this is where the :term:`FWHM`\ncomes from). The function :func:`nilearn.image.smooth_img` accounts for\npotential anisotropy in the image affine (i.e., non-indentical\n:term:`voxel` size in all the three dimensions). Analogous to the\nmajority of nilearn functions, :func:`nilearn.image.smooth_img` can\nalso use file names as input parameters.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Smooth the data using image processing module from nilearn\nfrom nilearn import image\n\n# Functional data\nfmri_filename = haxby_dataset.func[0]\n# smoothing: first argument as functional data filename and smoothing value\n# (integer) in second argument. Output returns in Nifti image.\nfmri_img = image.smooth_img(fmri_filename, fwhm=6)\n\n# Visualize the mean of the smoothed EPI image using plotting function\n# `plot_epi`\nfrom nilearn.plotting import plot_epi\n\n# First, compute the voxel-wise mean of smooth EPI image (first argument) using\n# image processing module `image`\nmean_img = image.mean_img(fmri_img)\n# Second, we visualize the mean image with coordinates positioned manually\nplot_epi(mean_img, title='Smoothed mean EPI', cut_coords=cut_coords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Given the smoothed functional data stored in variable 'fmri_img', we then\nselect two features of interest with face and house experimental conditions.\nThe method we will be using is a simple Student's t-test. The below section\ngives us brief motivation example about why selecting features in high\ndimensional FMRI data setting.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Functional MRI data can be considered \"high dimensional\" given the p-versus-n\nratio (e.g., p=~20,000-200,000 voxels for n=1000 samples or less). In this\nsetting, machine-learning algorithms can perform poorly due to the so-called\ncurse of dimensionality. However, simple means from the realms of classical\nstatistics can help reducing the number of voxels.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn.image import get_data\n\nfmri_data = get_data(fmri_img)\n# number of voxels being x*y*z, samples in 4th dimension\nfmri_data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Selecting features using T-test**: The Student's t-test\n(:func:`scipy.stats.ttest_ind`) is an established method to determine whether\ntwo distributions have a different mean value. It can be used to compare voxel\ntime-series from two different experimental conditions (e.g., when houses or\nfaces are shown to individuals during brain scanning). If the time-series\ndistribution is similar in the two conditions, then the voxel is not very\ninteresting to discriminate the condition.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nfrom scipy import stats\n\n# This test returns p-values that represent probabilities that the two\n# time-series were not drawn from the same distribution. The lower the\n# p-value, the more discriminative is the voxel in distinguishing the two\n# conditions (faces and houses).\n_, p_values = stats.ttest_ind(fmri_data[..., haxby_labels == 'face'],\n                              fmri_data[..., haxby_labels == 'house'],\n                              axis=-1)\n\n# Use a log scale for p-values\nlog_p_values = -np.log10(p_values)\n# NAN values to zero\nlog_p_values[np.isnan(log_p_values)] = 0.\nlog_p_values[log_p_values > 10.] = 10.\n\n# Visualize statistical p-values using plotting function `plot_stat_map`\nfrom nilearn.plotting import plot_stat_map\n\n# Before visualizing, we transform the computed p-values to Nifti-like image\n# using function `new_img_like` from nilearn.\nfrom nilearn.image import new_img_like\n\n# First argument being a reference image and second argument should be p-values\n# data to convert to a new image as output. This new image will have same header\n# information as reference image.\nlog_p_values_img = new_img_like(fmri_img, log_p_values)\n\n# Now, we visualize log p-values image on functional mean image as background\n# with coordinates given manually and colorbar on the right side of plot (by\n# default colorbar=True)\nplot_stat_map(log_p_values_img, mean_img,\n              title=\"p-values\", cut_coords=cut_coords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Selecting features using f_classif**: Feature selection method is also\navailable in the scikit-learn Python package, where it has been extended to\nseveral classes, using the `sklearn.feature_selection.f_classif` function.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build a mask from this statistical map (Improving the quality of the mask)\n**Thresholding** - We build the t-map to have better representation of voxels\nof interest and voxels with lower p-values correspond to the most intense\nvoxels. This can be done easily by applying a threshold to a t-map data in\narray.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Note that we use log p-values data; we force values below 5 to 0 by\n# thresholding.\nlog_p_values[log_p_values < 5] = 0\n\n# Visualize the reduced voxels of interest using statistical image plotting\n# function. As shown above, we first transform data in array to Nifti image.\nlog_p_values_img = new_img_like(fmri_img, log_p_values)\n\n# Now, visualizing the created log p-values to image without colorbar and\n# without Left - 'L', Right - 'R' annotation\nplot_stat_map(log_p_values_img, mean_img,\n              title='Thresholded p-values', annotate=False,\n              colorbar=False, cut_coords=cut_coords)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can post-process the results obtained with simple operations such as mask\nintersection and dilation to regularize the mask definition. The idea of using\nthese operations are to have more compact or sparser blobs.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Binarization** and **Intersection** with Ventral Temporal (VT) mask - We\nnow want to restrict our investigation to the VT area. The corresponding\nspatial mask is provided in haxby_dataset.mask_vt. We want to compute the\nintersection of this provided mask with our self-computed mask.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# self-computed mask\nbin_p_values = (log_p_values != 0)\n# VT mask\nmask_vt_filename = haxby_dataset.mask_vt[0]\n\n# The first step is to load VT mask and same time convert data type\n# numbers to boolean type\nfrom nilearn.image import load_img\n\nvt = get_data(load_img(mask_vt_filename)).astype(bool)\n\n# We can then use a logical \"and\" operation - numpy.logical_and - to keep only\n# voxels that have been selected in both masks. In neuroimaging jargon, this\n# is called an \"AND conjunction\". We use already imported numpy as np\nbin_p_values_and_vt = np.logical_and(bin_p_values, vt)\n\n# Visualizing the mask intersection results using plotting function `plot_roi`,\n# a function which can be used for visualizing target specific voxels.\nfrom nilearn.plotting import plot_roi, show\n\n# First, we create new image type of binarized and intersected mask (second\n# argument) and use this created Nifti image type in visualization. Binarized\n# values in data type boolean should be converted to int data type at the same\n# time. Otherwise, an error will be raised\nbin_p_values_and_vt_img = new_img_like(fmri_img,\n                                       bin_p_values_and_vt.astype(int))\n# Visualizing goes here with background as computed mean of functional images\nplot_roi(bin_p_values_and_vt_img, mean_img, cut_coords=cut_coords,\n         title='Intersection with ventral temporal mask')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Dilation** - Thresholded functional brain images often contain scattered\nvoxels across the brain. To consolidate such brain images towards more compact\nshapes, we use a [morphological dilation](http://en.wikipedia.org/wiki/Dilation_(morphology)). This is a common step\nto be sure not to forget voxels located on the edge of a ROI. In other words,\nsuch operations can fill \"holes\" in masked voxel representations.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# We use ndimage function from scipy Python library for mask dilation\nfrom scipy.ndimage import binary_dilation\n\n# Input here is a binarized and intersected mask data from previous section\ndil_bin_p_values_and_vt = binary_dilation(bin_p_values_and_vt)\n\n# Now, we visualize the same using `plot_roi` with data being converted to Nifti\n# image. In all new image like, reference image is the same but second argument\n# varies with data specific\ndil_bin_p_values_and_vt_img = new_img_like(\n    fmri_img,\n    dil_bin_p_values_and_vt.astype(int))\n# Visualization goes here without 'L', 'R' annotation and coordinates being the\n# same\nplot_roi(dil_bin_p_values_and_vt_img, mean_img,\n         title='Dilated mask', cut_coords=cut_coords,\n         annotate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we end with splitting the connected ROIs to two hemispheres into two\nseparate regions (ROIs). The function `scipy.ndimage.label` from the scipy\nPython library.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Identification of connected components** - The function\n:func:`scipy.ndimage.label` from the scipy Python library identifies\nimmediately neighboring voxels in our voxels mask. It assigns a separate\ninteger label to each one of them.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from scipy.ndimage import label\nlabels, n_labels = label(dil_bin_p_values_and_vt)\n# we take first roi data with labels assigned as integer 1\nfirst_roi_data = (labels == 5).astype(int)\n# Similarly, second roi data is assigned as integer 2\nsecond_roi_data = (labels == 3).astype(int)\n# Visualizing the connected components\n# First, we create a Nifti image type from first roi data in a array\nfirst_roi_img = new_img_like(fmri_img, first_roi_data)\n# Then, visualize the same created Nifti image in first argument and mean of\n# functional images as background (second argument), cut_coords is default now\n# and coordinates are selected automatically pointed exactly on the roi data\nplot_roi(first_roi_img, mean_img, title='Connected components: first ROI')\n# we do the same for second roi data\nsecond_roi_img = new_img_like(fmri_img, second_roi_data)\n# Visualization goes here with second roi image and cut_coords are default with\n# coordinates selected automatically pointed on the data\nplot_roi(second_roi_img, mean_img, title='Connected components: second ROI')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use the new ROIs, to extract data maps in both ROIs\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# We extract data from ROIs using nilearn's NiftiLabelsMasker\nfrom nilearn.maskers import NiftiLabelsMasker\n\n# Before data extraction, we convert an array labels to Nifti like image. All\n# inputs to NiftiLabelsMasker must be Nifti-like images or filename to Nifti\n# images. We use the same reference image as used above in previous sections\nlabels_img = new_img_like(fmri_img, labels)\n# First, initialize masker with parameters suited for data extraction using\n# labels as input image, resampling_target is None as affine, shape/size is same\n# for all the data used here, time series signal processing parameters\n# standardize and detrend are set to False\nmasker = NiftiLabelsMasker(labels_img, resampling_target=None,\n                           standardize=False, detrend=False)\n# After initialization of masker object, we call fit() for preparing labels_img\n# data according to given parameters\nmasker.fit()\n# Preparing for data extraction: setting number of conditions, size, etc from\n# haxby dataset\ncondition_names = haxby_labels.unique()\nn_cond_img = fmri_data[..., haxby_labels == 'house'].shape[-1]\nn_conds = len(condition_names)\n\nX1, X2 = np.zeros((n_cond_img, n_conds)), np.zeros((n_cond_img, n_conds))\n# Gathering data for each condition and then use transformer from masker\n# object transform() on each data. The transformer extracts data in condition\n# maps where the target regions are specified by labels images\nfor i, cond in enumerate(condition_names):\n    cond_maps = new_img_like(\n        fmri_img, fmri_data[..., haxby_labels == cond][..., :n_cond_img])\n    mask_data = masker.transform(cond_maps)\n    X1[:, i], X2[:, i] = mask_data[:, 0], mask_data[:, 1]\ncondition_names[np.where(condition_names == 'scrambledpix')] = 'scrambled'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "save the ROI 'atlas' to a Nifti file\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "new_img_like(fmri_img, labels).to_filename('mask_atlas.nii.gz')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot the average in the different condition names\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n\nplt.figure(figsize=(15, 7))\nfor i in np.arange(2):\n    plt.subplot(1, 2, i + 1)\n    plt.boxplot(X1 if i == 0 else X2)\n    plt.xticks(np.arange(len(condition_names)) + 1, condition_names,\n               rotation=25)\n    plt.title('Boxplots of data in ROI%i per condition' % (i + 1))\n\nshow()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}