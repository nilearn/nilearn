{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Advanced decoding using scikit learn\n\nThis tutorial opens the box of decoding pipelines to bridge integrated\nfunctionalities provided by the :class:`nilearn.decoding.Decoder` object\nwith more advanced usecases. It reproduces basic examples functionalities with\ndirect calls to scikit-learn function and gives pointers to more advanced\nobjects. If some concepts seem unclear, please refer to the `documentation on decoding <decoding_intro>`\nand in particular to the `advanced section <going_further>`.\nAs in many other examples, we perform decoding of the visual category of a\nstimuli on Haxby 2001 dataset, focusing on distinguishing two categories :\nface and cat images.\n\n    * J.V. Haxby et al. \"Distributed and Overlapping Representations of Faces\n      and Objects in Ventral Temporal Cortex\", Science vol 293 (2001), p\n      2425.-2430.\n\n.. include:: ../../../examples/masker_note.rst\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Retrieve and load the fMRI data from the Haxby study\n\n### First download the data\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# The :func:`nilearn.datasets.fetch_haxby` function will download the\n# Haxby dataset composed of fmri images in a Niimg, a spatial mask and a text\n# document with label of each image\nfrom nilearn import datasets\nhaxby_dataset = datasets.fetch_haxby()\nmask_filename = haxby_dataset.mask_vt[0]\nfmri_filename = haxby_dataset.func[0]\n# Loading the behavioral labels\nimport pandas as pd\nbehavioral = pd.read_csv(haxby_dataset.session_target[0], delimiter=' ')\nbehavioral"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We keep only a images from a pair of conditions(cats versus faces).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn.image import index_img\nconditions = behavioral['labels']\ncondition_mask = conditions.isin(['face', 'cat'])\nfmri_niimgs = index_img(fmri_filename, condition_mask)\nconditions = conditions[condition_mask]\n# Convert to numpy array\nconditions = conditions.values\nsession_label = behavioral['chunks'][condition_mask]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Performing decoding with scikit-learn\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Importing a classifier\n# ........................\n# We can import many predictive models from scikit-learn that can be used in a\n# decoding pipelines. They are all used with the same `fit()` and `predict()`\n# functions. Let's define a `Support Vector Classifier <http://scikit-learn.org/stable/modules/svm.html >`_ (or SVC).\nfrom sklearn.svm import SVC\nsvc = SVC()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Masking the data\nTo use a scikit-learn estimator on brain images, you should first mask the\ndata using a :class:`nilearn.maskers.NiftiMasker` to extract only the\nvoxels inside the mask of interest, and transform 4D input fMRI data to\n2D arrays(`shape=(n_timepoints, n_voxels)`) that estimators can work on.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn.maskers import NiftiMasker\nmasker = NiftiMasker(mask_img=mask_filename, runs=session_label,\n                     smoothing_fwhm=4, standardize=True,\n                     memory=\"nilearn_cache\", memory_level=1)\nfmri_masked = masker.fit_transform(fmri_niimgs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cross-validation with scikit-learn\nTo train and test the model in a meaningful way we use cross-validation with\nthe function :func:`sklearn.model_selection.cross_val_score` that computes\nfor you the score for the different folds of cross-validation.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score\n# Here `cv=5` stipulates a 5-fold cross-validation\ncv_scores = cross_val_score(svc, fmri_masked, conditions, cv=5)\nprint(\"SVC accuracy: {:.3f}\".format(cv_scores.mean()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tuning cross-validation parameters\nYou can change many parameters of the cross_validation here, for example:\n\n* use a different cross - validation scheme, for example LeaveOneGroupOut()\n\n* speed up the computation by using n_jobs = -1, which will spread the\n  computation equally across all processors.\n\n* use a different scoring function, as a keyword or imported from\n  scikit-learn : scoring = 'roc_auc'\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import LeaveOneGroupOut\ncv = LeaveOneGroupOut()\ncv_scores = cross_val_score(svc, fmri_masked, conditions, cv=cv,\n                            scoring='roc_auc', groups=session_label, n_jobs=-1)\nprint(\"SVC accuracy (tuned parameters): {:.3f}\".format(cv_scores.mean()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Measuring the chance level\n:class:`sklearn.dummy.DummyClassifier` (purely random) estimators are the\nsimplest way to measure prediction performance at chance. A more controlled\nway, but slower, is to do permutation testing on the labels, with\n:func:`sklearn.model_selection.permutation_test_score`.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dummy estimator\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.dummy import DummyClassifier\nnull_cv_scores = cross_val_score(\n    DummyClassifier(), fmri_masked, conditions, cv=cv, groups=session_label)\n\nprint(\"Dummy accuracy: {:.3f}\".format(null_cv_scores.mean()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Permutation test\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import permutation_test_score\nnull_cv_scores = permutation_test_score(\n    svc, fmri_masked, conditions, cv=cv, groups=session_label)[1]\nprint(\"Permutation test score: {:.3f}\".format(null_cv_scores.mean()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Decoding without a mask: Anova-SVM in scikit-lean\nWe can also implement feature selection before decoding as a scikit-learn\n`pipeline`(:class:`sklearn.pipeline.Pipeline`). For this, we need to import\nthe :mod:`sklearn.feature_selection` module and use\n:func:`sklearn.feature_selection.f_classif`, a simple F-score\nbased feature selection (a.k.a. [Anova](https://en.wikipedia.org/wiki/Analysis_of_variance#The_F-test)),\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\nfrom sklearn.feature_selection import SelectPercentile, f_classif\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.svm import LinearSVC\nfeature_selection = SelectPercentile(f_classif, percentile=10)\nanova_svc = Pipeline([('anova', feature_selection), ('svc', LinearSVC())])\n# We can use our ``anova_svc`` object exactly as we were using our ``svc``\n# object previously.\n# As we want to investigate our model, we use sklearn `cross_validate` function\n# with `return_estimator = True` instead of cross_val_score, to save the estimator\n\nfitted_pipeline = cross_validate(anova_svc, fmri_masked, conditions,\n                                 cv=cv, groups=session_label, return_estimator=True)\nprint(\n    \"ANOVA+SVC test score: {:.3f}\".format(fitted_pipeline[\"test_score\"].mean()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualize the ANOVA + SVC's discriminating weights\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# retrieve the pipeline fitted on the first cross-validation fold and its SVC\n# coefficients\nfirst_pipeline = fitted_pipeline[\"estimator\"][0]\nsvc_coef = first_pipeline.named_steps['svc'].coef_\nprint(\"After feature selection, the SVC is trained only on {} features\".format(\n    svc_coef.shape[1]))\n\n# We invert the feature selection step to put these coefs in the right 2D place\nfull_coef = first_pipeline.named_steps['anova'].inverse_transform(svc_coef)\n\nprint(\"After inverting feature selection, we have {} features back\".format(\n    full_coef.shape[1]))\n\n# We apply the inverse of masking on these to make a 4D image that we can plot\nfrom nilearn.plotting import plot_stat_map\nweight_img = masker.inverse_transform(full_coef)\nplot_stat_map(weight_img, title='Anova+SVC weights')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Going further with scikit-learn\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Changing the prediction engine\nTo change the prediction engine, we just need to import it and use in our\npipeline instead of the SVC. We can try Fisher's [Linear Discriminant Analysis (LDA)](http://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_vs_lda.html)\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Construct the new estimator object and use it in a new pipeline after anova\n\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfeature_selection = SelectPercentile(f_classif, percentile=10)\nlda = LinearDiscriminantAnalysis()\nanova_lda = Pipeline([('anova', feature_selection), ('LDA', lda)])\n\n# Recompute the cross-validation score:\nimport numpy as np\ncv_scores = cross_val_score(anova_lda, fmri_masked,\n                            conditions, cv=cv, verbose=1, groups=session_label)\nclassification_accuracy = np.mean(cv_scores)\nn_conditions = len(set(conditions))  # number of target classes\nprint(\"ANOVA + LDA classification accuracy: %.4f / Chance Level: %.4f\" %\n      (classification_accuracy, 1. / n_conditions))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Changing the feature selection\nLet's say that you want a more sophisticated feature selection, for example a\nRecursive Feature Elimination(RFE) before a svc. We follows the same principle.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Import it and define your fancy objects\nfrom sklearn.feature_selection import RFE\nsvc = SVC()\nrfe = RFE(SVC(kernel='linear', C=1.), n_features_to_select=50, step=0.25)\n\n# Create a new pipeline, composing the two classifiers `rfe` and `svc`\n\nrfe_svc = Pipeline([('rfe', rfe), ('svc', svc)])\n\n# Recompute the cross-validation score\n# cv_scores = cross_val_score(rfe_svc, fmri_masked, target, cv=cv, n_jobs=-1, verbose=1)\n# But, be aware that this can take * A WHILE * ..."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}