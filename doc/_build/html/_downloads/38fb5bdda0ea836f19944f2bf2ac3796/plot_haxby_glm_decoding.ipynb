{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Decoding of a dataset after GLM fit for signal extraction\n\nFull step-by-step example of fitting a GLM to perform a decoding experiment.\nWe use the data from one subject of the Haxby dataset.\n\nMore specifically:\n\n1. Download the Haxby dataset.\n2. Extract the information to generate a glm representing the blocks of stimuli.\n3. Analyze the decoding performance using a classifier.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fetch example Haxby dataset\nWe download the Haxby dataset\nThis is a study of visual object category representation\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# By default 2nd subject will be fetched\nimport numpy as np\nimport pandas as pd\nfrom nilearn import datasets\nhaxby_dataset = datasets.fetch_haxby()\n\n# repetition has to be known\nTR = 2.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load the behavioral data\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Load target information as string and give a numerical identifier to each\nbehavioral = pd.read_csv(haxby_dataset.session_target[0], sep=' ')\nconditions = behavioral['labels'].values\n\n# Record these as an array of sessions\nsessions = behavioral['chunks'].values\nunique_sessions = behavioral['chunks'].unique()\n\n# fMRI data: a unique file for each session\nfunc_filename = haxby_dataset.func[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build a proper event structure for each session\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "events = {}\n# events will take  the form of a dictionary of Dataframes, one per session\nfor session in unique_sessions:\n    # get the condition label per session\n    conditions_session = conditions[sessions == session]\n    # get the number of scans per session, then the corresponding\n    # vector of frame times\n    n_scans = len(conditions_session)\n    frame_times = TR * np.arange(n_scans)\n    # each event last the full TR\n    duration = TR * np.ones(n_scans)\n    # Define the events object\n    events_ = pd.DataFrame(\n        {'onset': frame_times, 'trial_type': conditions_session, 'duration': duration})\n    # remove the rest condition and insert into the dictionary\n    events[session] = events_[events_.trial_type != 'rest']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Instantiate and run FirstLevelModel\n\nWe generate a list of z-maps together with their session and condition index\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "z_maps = []\nconditions_label = []\nsession_label = []\n\n# Instantiate the glm\nfrom nilearn.glm.first_level import FirstLevelModel\nglm = FirstLevelModel(t_r=TR,\n                      mask_img=haxby_dataset.mask,\n                      high_pass=.008,\n                      smoothing_fwhm=4,\n                      memory='nilearn_cache')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run the glm on data from each session\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "events[session].trial_type.unique()\nfrom nilearn.image import index_img\nfor session in unique_sessions:\n    # grab the fmri data for that particular session\n    fmri_session = index_img(func_filename, sessions == session)\n\n    # fit the glm\n    glm.fit(fmri_session, events=events[session])\n\n    # set up contrasts: one per condition\n    conditions = events[session].trial_type.unique()\n    for condition_ in conditions:\n        z_maps.append(glm.compute_contrast(condition_))\n        conditions_label.append(condition_)\n        session_label.append(session)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generating a report\nSince we have already computed the FirstLevelModel\nand have the contrast, we can quickly create a summary report.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn.image import mean_img\nfrom nilearn.reporting import make_glm_report\nmean_img_ = mean_img(func_filename)\nreport = make_glm_report(glm,\n                         contrasts=conditions,\n                         bg_img=mean_img_,\n                         )\n\nreport  # This report can be viewed in a notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In a jupyter notebook, the report will be automatically inserted, as above.\nWe have several other ways to access the report:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# report.save_as_html('report.html')\n# report.open_in_browser()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build the decoding pipeline\nTo define the decoding pipeline we use Decoder object, we choose :\n\n    * a prediction model, here a Support Vector Classifier, with a linear\n      kernel\n\n    * the mask to use, here a ventral temporal ROI in the visual cortex\n\n    * although it usually helps to decode better, z-maps time series don't\n      need to be rescaled to a 0 mean, variance of 1 so we use\n      standardize=False.\n\n    * we use univariate feature selection to reduce the dimension of the\n      problem keeping only 5% of voxels which are most informative.\n\n    * a cross-validation scheme, here we use LeaveOneGroupOut\n      cross-validation on the sessions which corresponds to a\n      leave-one-session-out\n\nWe fit directly this pipeline on the Niimgs outputs of the GLM, with\ncorresponding conditions labels and session labels (for the cross validation).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn.decoding import Decoder\nfrom sklearn.model_selection import LeaveOneGroupOut\ndecoder = Decoder(estimator='svc', mask=haxby_dataset.mask, standardize=False,\n                  screening_percentile=5, cv=LeaveOneGroupOut())\ndecoder.fit(z_maps, conditions_label, groups=session_label)\n\n# Return the corresponding mean prediction accuracy compared to chance\n\nclassification_accuracy = np.mean(list(decoder.cv_scores_.values()))\nchance_level = 1. / len(np.unique(conditions))\nprint('Classification accuracy: {:.4f} / Chance level: {}'.format(\n    classification_accuracy, chance_level))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}