{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Example of surface-based first-level analysis\n\nA full step-by-step example of fitting a GLM to experimental data sampled on\nthe cortical surface and visualizing the results.\n\nMore specifically:\n\n1. A sequence of fMRI volumes is loaded.\n2. fMRI data are projected onto a reference cortical surface (the FreeSurfer\n   template, fsaverage).\n3. A design matrix describing all the effects related to the data is computed.\n4. A GLM is applied to the dataset (effect/covariance, then contrast estimation).\n\nThe result of the analysis are statistical maps that are defined on the brain\nmesh. We display them using Nilearn capabilities.\n\nThe projection of fMRI data onto a given brain mesh requires that both are\ninitially defined in the same space.\n\n* The functional data should be coregistered to the anatomy from which the mesh\n  was obtained.\n\n* Another possibility, used here, is to project the normalized fMRI data to an\n  MNI-coregistered mesh, such as fsaverage.\n\nThe advantage of this second approach is that it makes it easy to run\nsecond-level analyses on the surface. On the other hand, it is obviously less\naccurate than using a subject-tailored mesh.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare data and analysis parameters\nPrepare the timing parameters.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "t_r = 2.4\nslice_time_ref = 0.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prepare the data.\nFirst, the volume-based fMRI data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn.datasets import fetch_localizer_first_level\ndata = fetch_localizer_first_level()\nfmri_img = data.epi_img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Second, the experimental paradigm.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "events_file = data.events\nimport pandas as pd\nevents = pd.read_table(events_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Project the fMRI image to the surface\n\nFor this we need to get a mesh representing the geometry of the surface. We\ncould use an individual mesh, but we first resort to a standard mesh, the\nso-called fsaverage5 template from the FreeSurfer software.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import nilearn\nfsaverage = nilearn.datasets.fetch_surf_fsaverage()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The projection function simply takes the fMRI data and the mesh.\nNote that those correspond spatially, are they are both in MNI space.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn import surface\ntexture = surface.vol_to_surf(fmri_img, fsaverage.pial_right)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Perform first level analysis\n\nThis involves computing the design matrix and fitting the model.\nWe start by specifying the timing of fMRI frames.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nn_scans = texture.shape[1]\nframe_times = t_r * (np.arange(n_scans) + .5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create the design matrix.\n\nWe specify an hrf model containing the Glover model and its time derivative\nThe drift model is implicitly a cosine basis with a period cutoff at 128s.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn.glm.first_level import make_first_level_design_matrix\ndesign_matrix = make_first_level_design_matrix(frame_times,\n                                               events=events,\n                                               hrf_model='glover + derivative'\n                                               )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Setup and fit GLM.\n\nNote that the output consists in 2 variables: `labels` and `fit`.\n`labels` tags voxels according to noise autocorrelation.\n`estimates` contains the parameter estimates.\nWe keep them for later contrast computation.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn.glm.first_level import run_glm\nlabels, estimates = run_glm(texture.T, design_matrix.values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Estimate contrasts\nSpecify the contrasts.\n\nFor practical purpose, we first generate an identity matrix whose size is\nthe number of columns of the design matrix.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "contrast_matrix = np.eye(design_matrix.shape[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "At first, we create basic contrasts.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "basic_contrasts = dict([(column, contrast_matrix[i])\n                        for i, column in enumerate(design_matrix.columns)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we add some intermediate contrasts and\none contrast adding all conditions with some auditory parts.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "basic_contrasts['audio'] = (\n    basic_contrasts['audio_left_hand_button_press']\n    + basic_contrasts['audio_right_hand_button_press']\n    + basic_contrasts['audio_computation']\n    + basic_contrasts['sentence_listening'])\n\n# one contrast adding all conditions involving instructions reading\nbasic_contrasts['visual'] = (\n    basic_contrasts['visual_left_hand_button_press']\n    + basic_contrasts['visual_right_hand_button_press']\n    + basic_contrasts['visual_computation']\n    + basic_contrasts['sentence_reading'])\n\n# one contrast adding all conditions involving computation\nbasic_contrasts['computation'] = (basic_contrasts['visual_computation']\n                                  + basic_contrasts['audio_computation'])\n\n# one contrast adding all conditions involving sentences\nbasic_contrasts['sentences'] = (basic_contrasts['sentence_listening']\n                                + basic_contrasts['sentence_reading'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we create a dictionary of more relevant contrasts\n\n* 'left - right button press': probes motor activity in left versus right button presses.\n* 'audio - visual': probes the difference of activity between listening to some content or reading the same type of content (instructions, stories).\n* 'computation - sentences': looks at the activity when performing a mental computation task  versus simply reading sentences.\n\nOf course, we could define other contrasts, but we keep only 3 for simplicity.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "contrasts = {\n    'left - right button press': (\n        basic_contrasts['audio_left_hand_button_press']\n        - basic_contrasts['audio_right_hand_button_press']\n        + basic_contrasts['visual_left_hand_button_press']\n        - basic_contrasts['visual_right_hand_button_press']\n    ),\n    'audio - visual': basic_contrasts['audio'] - basic_contrasts['visual'],\n    'computation - sentences': (basic_contrasts['computation'] -\n                                basic_contrasts['sentences']\n    )\n}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's estimate the contrasts by iterating over them.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn.glm.contrasts import compute_contrast\nfrom nilearn import plotting\n\nfor index, (contrast_id, contrast_val) in enumerate(contrasts.items()):\n    print('  Contrast % i out of %i: %s, right hemisphere' %\n          (index + 1, len(contrasts), contrast_id))\n    # compute contrast-related statistics\n    contrast = compute_contrast(labels, estimates, contrast_val,\n                                contrast_type='t')\n    # we present the Z-transform of the t map\n    z_score = contrast.z_score()\n    # we plot it on the surface, on the inflated fsaverage mesh,\n    # together with a suitable background to give an impression\n    # of the cortex folding.\n    plotting.plot_surf_stat_map(\n        fsaverage.infl_right, z_score, hemi='right',\n        title=contrast_id, colorbar=True,\n        threshold=3., bg_map=fsaverage.sulc_right)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analysing the left hemisphere\n\nNote that re-creating the above analysis for the left hemisphere requires\nlittle additional code!\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We project the fMRI data to the mesh.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "texture = surface.vol_to_surf(fmri_img, fsaverage.pial_left)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we estimate the General Linear Model.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "labels, estimates = run_glm(texture.T, design_matrix.values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we create contrast-specific maps and plot them.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for index, (contrast_id, contrast_val) in enumerate(contrasts.items()):\n    print('  Contrast % i out of %i: %s, left hemisphere' %\n          (index + 1, len(contrasts), contrast_id))\n    # compute contrasts\n    contrast = compute_contrast(labels, estimates, contrast_val,\n                                contrast_type='t')\n    z_score = contrast.z_score()\n    # plot the result\n    plotting.plot_surf_stat_map(\n        fsaverage.infl_left, z_score, hemi='left',\n        title=contrast_id, colorbar=True,\n        threshold=3., bg_map=fsaverage.sulc_left)\n\nplotting.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}